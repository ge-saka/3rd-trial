# Group-12-Final-NLP-Project

## Toxic Language Detection System

![WhatsApp Image 2024-08-08 at 23 31 44_3c638a92](https://github.com/user-attachments/assets/1c186f77-4d62-491c-8379-9e5ccda97b32)

 ### Overview :
 The 'Toxic Language Detection System' project leverages the Toxigen dataset to analyze sentiments expressed online, particularly those targeted at minority groups. The aim is to refine the detection of toxic or negative comments, ensuring that harmful content is accurately identified and flagged by advanced machine learning models. This system is crucial in creating safer online environments, particularly for minority communities who are often the targets of online toxicity.
 
### Business Understanding :
The rise of online platforms has provided a space for both positive and negative interactions. Unfortunately, this has also led to the proliferation of toxic behaviors such as cyberbullying, hate speech, and harassment, which have serious real-world consequences. For businesses, particularly social media companies, the accurate detection of toxic language is vital in maintaining user trust and safety. The primary objective of this project is to develop a robust classifier that can detect toxic comments with high accuracy, thereby helping platforms manage and mitigate the impact of online toxicity.

### Problem Statement :
Online toxicity, characterized by negative and harmful behaviors in digital interactions, has become a significant challenge in the modern digital landscape. This toxicity often leads to psychological harm, mental health issues, and societal division, particularly among minority groups. The project addresses the challenge of detecting toxic language online, ensuring that detection systems are fair, unbiased, and effective in identifying harmful content without disproportionately targeting minority groups.

### Data Understanding :
The Toxigen dataset consists of 250,951 rows and 6 columns. It includes toxic and benign statements about 13 minority groups. Key variables in the dataset include:
 • Prompt: The original message or sentiment.
 • Generation: The response generated by a modeling tool after the prompt.
 • Group: The minority group targeted by the comments.
 • Prompt Label & Roberta Prediction: Indications of whether the statement is toxic or benign.
The dataset required preprocessing, including the removal of duplicates, handling of missing values, and cleaning of text data (e.g., removing stop words and special characters). Feature engineering was also applied to enhance the model's ability to detect toxic language.

### Exploratory Data Analysis (EDA) :
Key findings from the EDA include:
• Group Distribution: The dataset included 13 distinct minority groups, with the LGBTQ group having the highest representation (8.3%) and physically disabled the least (6.2%).
• Prompt and Generation Length: The distribution of comment lengths was analyzed, revealing a lack of symmetry in the 'Generation Length' but some level of symmetry in 'Prompt Length.'
• Sentiment Analysis: Sentiment analysis was performed using both TextBlob and VADER. The sentiment of comments was categorized as positive, neutral, or negative. For instance, the sentiment analysis revealed that negative comments dominated the dataset, with 136k negative prompts and 64k negative generated responses.

### Visualizations :
Visualizations were used extensively to convey insights, including:
• Bar Plots: Showcasing the distribution of minority groups and sentiment categories.
• Histograms: Illustrating the distribution of sentiment scores across different groups.
• Box Plots: Highlighting the variation in prompt and generation lengths by group.
• Word Clouds: Providing a visual representation of the most frequently occurring words in the dataset.
• Kernel Density Estimation: Used to visualize the distribution of sentiment scores for each minority group.
Example: Sentiment category distribution for prompted texts, showing a higher proportion of negative sentiments.

### Modelling :
The project employed several machine learning models to detect toxic language. Key models include:
• Logistic Regression: After initial training, the model achieved a high accuracy of 96%. However, hyperparameter tuning using GridSearchCV improved the model further, achieving an accuracy of 99%.
• Latent Dirichlet Allocation (LDA): Used for topic modeling to categorize comments into positive, negative, or neutral categories. The LDA model helped uncover underlying themes in the text data.
Model evaluation showed high precision and recall, particularly for the 'Negative' and 'Positive' classes, indicating that the model was well-tuned and effective in detecting toxicity.

### Recommendations :
Based on the analysis and model performance, the following recommendations are made:
• Implement Community Guidelines: Enforce strict community guidelines to prevent and address toxic behavior. Platforms like Instagram can serve as a model, using AI to detect and moderate harmful comments.
• Counseling Services: Provide access to counseling for individuals affected by online toxicity to mitigate psychological harm.
• Legal Frameworks: Advocate for stricter legal frameworks to address cyberbullying and hate speech effectively.
• Continuous Model Monitoring: Regularly update and monitor the detection models to adapt to new patterns of toxicity and reduce bias against minority groups.

### Conclusion :
The 'Toxic Language Detection System' project successfully developed and refined a robust classifier capable of detecting toxic language with high accuracy. The models, supported by comprehensive data analysis and feature engineering, are well-suited to be implemented in real-world platforms to safeguard minority groups from online harm. Continuous monitoring and improvement of these models are essential to ensure their effectiveness in an ever-evolving digital landscape.
